##### solver functions #####
using Roots
using LinearAlgebra
"""
Parameter Configurations
X:  observations
y:  responses
Œ∏:  variables
w:  weight-parameter for Œ∏ (for trimmed penalty)
Œª:  coefficient of regularizer
œÅ:  parameter for nonconvex penalties, SCAD and MCP
Œ∫:  proximal parameters (Œ∫ = Œ∑*Œª)
œÑ:  stepsize for w
Œ±:  belief of sparsity level
h:  trimming parameter
"""

# Just utils
function eye(p)
    I = zeros(p, p)
    for i = 1:p
        I[i, i] = 1.0
    end
    return I
end

# Since the objective is "sum", the corresponding Œª value should be scaled
function trim_objective(X, y, Œ∏, w, Œª)
    return 0.5*sum(abs2, X*Œ∏ - y) + Œª*sum(abs, Œ∏.*w)
end

function scad_objective(X, y, Œ∏, œÅ, Œª)
    return 0.5*sum(abs2, X*Œ∏ - y) + Œª*scad(Œ∏, Œª, œÅ)
end

function mcp_objective(X, y, Œ∏, œÅ, Œª)
    return 0.5*sum(abs2, X*Œ∏ - y) + Œª*mcp(Œ∏, Œª, œÅ)
end

function vanilla_objective(X, y, Œ∏, Œª)
    return 0.5*sum(abs2, X*Œ∏ - y) + Œª*sum(abs, Œ∏)
end

function scad(Œ∏, Œª, œÅ)
    val = 0.0
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) ‚â§ Œª ? val += Œª*abs(Œ∏[i]) :
        abs(Œ∏[i]) > œÅ*Œª ? val += 0.5*(œÅ+1)*Œª^2 :
        val += (2*œÅ*Œª*abs(Œ∏[i]) - Œª^2 - Œ∏[i]^2) / (2*(œÅ-1))
    end
    return val
end

function mcp(Œ∏, Œª, œÅ)
    val = 0.0
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) ‚â§ œÅ*Œª ? val += Œª*abs(Œ∏[i]) - Œ∏[i]^2/(2*œÅ) :
        val += 0.5*œÅ*Œª^2
    end
    return val
end

function prox_‚Ñì‚ÇÄ_norm(Œ∏, Œ∫)
    ŒΩ = sqrt(2.0*Œ∫)
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) ‚â§ ŒΩ && (Œ∏[i] == 0.0);
    end
end

function prox_‚Ñì‚ÇÅ_norm(Œ∏, Œ∫)
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) > Œ∫ ? Œ∏[i] = sign(Œ∏[i])*max(abs(Œ∏[i])-Œ∫, 0) :
        Œ∏[i] = 0.0
    end
end

function prox_trimmed_‚Ñì‚ÇÅ_norm(Œ∏, w, Œ∫)
    for i in eachindex(Œ∏)
        ŒΩ = Œ∫*w[i]
        abs(Œ∏[i]) > ŒΩ ? Œ∏[i] = sign(Œ∏[i])*max(abs(Œ∏[i])-ŒΩ, 0) :
        Œ∏[i] = 0.0
    end
end

function prox_‚Ñì‚ÇÅ‚ÇÇ_norm(Œ∏, w, Œ∫)
    for i = 1:length(w)
        ŒΩ = Œ∫*w[i]
        œÅ = vecnorm(x[i, :]); r = œÅ;
        œÅ == 0.0 && continue;
        œÅ > ŒΩ ? r -= ŒΩ : r = 0.0;
        x[i, :] = (r/œÅ) * x[i, :]
    end
end

function prox_capped_simplex(w, lb, ub, h)
    h == length(w) && (fill!(w, 1.0); return nothing;)
    h  > length(w) && error("wrong h or size of vector!");

    h = Float64(h)
    f(Œ±) = sum(max.(min.(w .- Œ±, ub), lb)) - h
    a = -1.5 + minimum(w)
    b = maximum(w)
    Œ± = fzero(f, [a, b])
    for i in eachindex(w)
        w[i] -= Œ±
        w[i] > ub ? w[i] = ub :
        w[i] < lb ? w[i] = lb :
        nothing
    end
end

function prox_scad(Œ∏, œÅ, Œ∫)
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) < 2*Œ∫ ? Œ∏[i] = sign(Œ∏[i])*max(abs(Œ∏[i])-Œ∫, 0) :
        abs(Œ∏[i]) > œÅ*Œ∫ ? Œ∏[i] = Œ∏[i] :
        Œ∏[i] = ((œÅ-1)*Œ∏[i] - sign(Œ∏[i])*œÅ*Œ∫)/(œÅ-2)
    end
end

function prox_mcp(Œ∏, œÅ, Œ∫)
    for i in eachindex(Œ∏)
        abs(Œ∏[i]) > œÅ*Œ∫ ? Œ∏[i] = Œ∏[i] :
        abs(Œ∏[i]) ‚â§ Œ∫ ? Œ∏[i] = 0.0 :
        Œ∏[i] = (œÅ*Œ∏[i] - sign(Œ∏[i])*œÅ*Œ∫)/(œÅ-1)
    end
end

# From here, we use vanilla gradient descent, ‚àáŒ∏ = ‚àÇùìõ/‚àÇŒ∏, Œ∏‚ÅΩ·µó‚Å∫¬π‚Åæ = Œ∏‚ÅΩ·µó‚Åæ - Œ∑‚àáŒ∏
function solve_trim_lasso(X, y, Œ∏, w, h, Œª; itm=1000, tol=1e-6, ptf=100)
    Œ∏‚Åª = copy(Œ∏)
    Œ∑  = 1.0/opnorm(X)^2
    Œ∫  = Œ∑*Œª
    œÑ  = 1.0

    err = 1.0
    noi = 0

    while err ‚â• tol
        # proximal gradient on Œ∏
        ‚àáŒ∏ = X'*(X*Œ∏ - y)
        BLAS.axpy!(-Œ∑, ‚àáŒ∏, Œ∏)
        prox_trimmed_‚Ñì‚ÇÅ_norm(Œ∏, w, Œ∫)

        # projected gradient on w
        BLAS.axpy!(-œÑ, abs.(Œ∏), w)
        prox_capped_simplex(w, 0.0, 1.0, h)
        
        # update informations
        obj = trim_objective(X, y, Œ∏, w, Œª)
        err = norm(Œ∏‚Åª - Œ∏)/Œ∑
        copyto!(Œ∏‚Åª, Œ∏)

        # print learning information
        noi += 1
        #noi % ptf == 0 && @printf("iter %4d, obj %1.2e, err %1.2e\n",
        #                          noi, obj, err);
        noi ‚â• itm && break;
    end
end

function solve_mcp_lasso(X, y, Œ∏, œÅ, Œª; itm=1000, tol=1e-6, ptf=100)
    Œ∏‚Åª = copy(Œ∏)
    Œ∑  = 1.0/opnorm(X)^2
    Œ∫  = Œ∑*Œª

    err = 1.0
    noi = 0

    while err ‚â• tol
        ‚àáŒ∏ = X'*(X*Œ∏ - y)
        BLAS.axpy!(-Œ∑, ‚àáŒ∏, Œ∏)
        prox_mcp(Œ∏, œÅ, Œ∫)

        obj = mcp_objective(X, y, Œ∏, œÅ, Œª)
        err = norm(Œ∏‚Åª - Œ∏)/Œ∑
        copyto!(Œ∏‚Åª, Œ∏)

        noi += 1
        #noi % ptf == 0 && @printf("iter %4d, obj %1.2e, err %1.2e\n",
        #                          noi, obj, err);
        noi ‚â• itm && break;
    end
end

function solve_scad_lasso(X, y, Œ∏, œÅ, Œª; itm=1000, tol=1e-6, ptf=100)
    Œ∏‚Åª = copy(Œ∏)
    Œ∑  = 1.0/opnorm(X)^2
    Œ∫  = Œ∑*Œª

    err = 1.0
    noi = 0

    while err ‚â• tol
        ‚àáŒ∏ = X'*(X*Œ∏ - y)
        BLAS.axpy!(-Œ∑, ‚àáŒ∏, Œ∏)
        prox_scad(Œ∏, œÅ, Œ∫)

        obj = scad_objective(X, y, Œ∏, œÅ, Œª)
        err = norm(Œ∏‚Åª - Œ∏)/Œ∑
        copyto!(Œ∏‚Åª, Œ∏)

        noi += 1
        #noi % ptf == 0 && @printf("iter %4d, obj %1.2e, err %1.2e\n",
        #                          noi, obj, err);
        noi ‚â• itm && break;
    end
end

function solve_vanilla_lasso(X, y, Œ∏, Œª; itm=1000, tol=1e-6, ptf=100)
    Œ∏‚Åª = copy(Œ∏)
    Œ∑  = 1.0/opnorm(X)^2
    Œ∫  = Œ∑*Œª

    err = 1.0
    noi = 0
    
    while err ‚â• tol
        ‚àáŒ∏ = X'*(X*Œ∏ - y)
        BLAS.axpy!(-Œ∑, ‚àáŒ∏, Œ∏)
        prox_‚Ñì‚ÇÅ_norm(Œ∏, Œ∫)

        obj = vanilla_objective(X, y, Œ∏, Œª)
        err = norm(Œ∏‚Åª - Œ∏)/Œ∑
        copyto!(Œ∏‚Åª, Œ∏)

        noi += 1
        #noi % ptf == 0 && @printf("iter %4d, obj %1.2e, err %1.2e\n",
        #                          noi, obj, err);
        noi ‚â• itm && break
    end
end
